# 5.2 Refactor: Make it Better

## 1. Contoso Chat Sample

Once you've familiarized yourself with the basic setup, ideation and evaluation capabilities, you can explore the [Contoso Chat](https://aka.ms/aitour/contoso-chat) sample for more advanced insights, on your own. 

This sample uses the same retail dataset used in this RAG Chat workshop, but streamlines the end-to-end development workflow with core developer tools like Azure Developer CLI, to automate provisioning and deployment. The sample also teaches you how to package up your application prototype and deploy it into production using Azure Container Apps.

![Workshop v3](https://nitya.github.io/contoso-chat/img/workshop-developer-flow.png)

!!! warning "Contoso Chat is currently in v3. An updated v4 (Jan 2025) will reflect recent Azure AI SDK updates"

## 2. RAG Chat Sample (v2)

Another option is to use this existing repository and workshop _as a sandbox sample_ that you can customize to your own requirements. Here are some suggestions for ways to experiment further:

1. **Bring Your Own Data** - Walk through the steps but replace the `product.csv` with your data, and understand the implications for all the other steps in the workflow. For example - you will need to create a new set of evaluation test inputs (for your use case) and refactor the prompt templates (for intent mapping and chat-with-products) to reflect new instructions or context.

1. **Integrate More Sources** - The basic sample has a single data source (product catalog index). However, the `data/` folder provides additional data sources (customer data, product manual data) that can be used for even richer experiences. Think of how you can refactor the existing code to support _data retrieval_ from multiple sources that store this data (e.g., Customer data in Azure Cosmos DB).

3. **Write Custom Evaluators** - The default evaluators for quality reflect common best practices. However, you may want to assess responses for a _custom criteria_. Think of how you can write a custom evaluator, and add it into the evalution script. For example: evaluate responses for _politeness_ or for being _kid-friendly_.

4. **Explore Richer RAG Patterns** - The core of RAG patterns is the _retrieval_ of relevant knowledge from various sources including Azure AI Search. Explore new algorithms or ways of combining context to make the retrieved documents more relevant to application needs.

5. **Make it Agentic** - Agentic workflows are dominating the AI ecosystem today. Agentic RAG refers to the use of AI agents for information retrieval where the agent mediates between multiple data sources to drive enhanced RAG pipelines for delivery. [Read more here](https://weaviate.io/blog/what-is-agentic-rag).